{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import pytorch_mask_rcnn as pmr\n",
    "    \n",
    "    \n",
    "def main(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.use_cuda else \"cpu\")\n",
    "    cuda = device.type == \"cuda\"\n",
    "    if cuda: pmr.get_gpu_prop(show=True)\n",
    "    print(\"\\ndevice: {}\".format(device))\n",
    "    \n",
    "    d_test = pmr.datasets(args.dataset, args.data_dir, \"val\", train=True) # VOC 2012. set train=True for eval\n",
    "    #d_test = pmr.datasets(args.dataset, args.data_dir, \"val2017\", train=True) # COCO 2017\n",
    "\n",
    "    print(args)\n",
    "    num_classes = max(d_test.classes) + 1\n",
    "    model = pmr.maskrcnn_resnet50(False, num_classes).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(args.ckpt_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    #print(checkpoint[\"eval_info\"])\n",
    "    del checkpoint\n",
    "    if cuda: torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\nevaluating...\\n\")\n",
    "    \n",
    "    B = time.time()\n",
    "    eval_output, iter_eval = pmr.evaluate(model, d_test, device, args)\n",
    "    B = time.time() - B\n",
    "    \n",
    "    print(eval_output.get_AP())\n",
    "    if iter_eval is not None:\n",
    "        print(\"\\nTotal time of this evaluation: {:.1f} s, speed: {:.1f} imgs/s\".format(B, 1 / iter_eval))\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dataset\", default=\"voc\")\n",
    "    parser.add_argument(\"--data-dir\", default=\"E:/PyTorch/data/voc2012\")\n",
    "    parser.add_argument(\"--ckpt-path\", default=\"E:/PyTorch/scripts/ckpts/maskrcnn_voc-5.pth\")\n",
    "    parser.add_argument(\"--iters\", type=int, default=3) # number of iterations, minus means the entire dataset\n",
    "    args = parser.parse_args([]) # [] is needed if you're using Jupyter Notebook.\n",
    "    \n",
    "    args.use_cuda = True\n",
    "    args.results = os.path.join(os.path.dirname(args.ckpt_path), \"maskrcnn_results.pth\")\n",
    "    \n",
    "    main(args)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "device: cpu\n",
      "loading annotations into memory...\n",
      "Done (t=1.56s)\n",
      "creating index...\n",
      "index created!\n",
      "Namespace(ckpt_path='/home/moka/Akash/PyTorch-Simple-MaskRCNN/maskrcnn_coco-3.pth', data_dir='/home/moka/Akash/PyTorch-Simple-MaskRCNN/data', dataset='coco', iters=3, results='/home/moka/Akash/PyTorch-Simple-MaskRCNN/maskrcnn_results.pth', use_cuda=True)\n",
      "torch.Size([3, 426, 640])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MaskRCNN' object has no attribute 'clone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m args\u001b[38;5;241m.\u001b[39muse_cuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     72\u001b[0m args\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(args\u001b[38;5;241m.\u001b[39mckpt_path), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaskrcnn_results.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Step 2: Define dummy input (replace this with your actual input dimensions)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# dummy_input = torch.randn(3, 224, 224)#.to(device)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m sample_input_int \u001b[38;5;241m=\u001b[39m sample_input\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 39\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Step 3: Export the PyTorch model to ONNX format\u001b[39;00m\n\u001b[1;32m     43\u001b[0m onnx_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Akash/PyTorch-Simple-MaskRCNN/maskrcnn38/lib/python3.8/site-packages/torch/nn/modules/module.py:1177\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1176\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1177\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MaskRCNN' object has no attribute 'clone'"
     ]
    }
   ],
   "source": [
    " \n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import pytorch_mask_rcnn as pmr\n",
    "import torch\n",
    "import torch.onnx as onnx\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image   \n",
    "    \n",
    "def main(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.use_cuda else \"cpu\")\n",
    "    cuda = device.type == \"cuda\"\n",
    "    if cuda: pmr.get_gpu_prop(show=True)\n",
    "    print(\"\\ndevice: {}\".format(device))\n",
    "    \n",
    "    # d_test = pmr.datasets(args.dataset, args.data_dir, \"val\", train=True) # VOC 2012. set train=True for eval\n",
    "    d_test = pmr.datasets(args.dataset, args.data_dir, \"val2017\", train=True) # COCO 2017\n",
    "\n",
    "    print(args)\n",
    "    num_classes = max(d_test.classes) + 1\n",
    "    model = pmr.maskrcnn_resnet50(False, num_classes).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(args.ckpt_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    #print(checkpoint[\"eval_info\"])\n",
    "\n",
    "    # Load a sample image from your dataset to create a dummy input\n",
    "    sample_image, _ = d_test[0]\n",
    "    sample_input = sample_image.to(device)\n",
    "    print(sample_input.size())\n",
    "    # Step 2: Define dummy input (replace this with your actual input dimensions)\n",
    "    # dummy_input = torch.randn(3, 224, 224)#.to(device)\n",
    "    sample_input_int = sample_input.float()\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "\n",
    "    # Step 3: Export the PyTorch model to ONNX format\n",
    "    onnx_path = \"model.onnx\"\n",
    "    torch.onnx.export(model, sample_input_int, onnx_path, verbose=True, input_names=[\"input\"], output_names=[\"output\"])\n",
    "\n",
    "    # Step 4: Load the ONNX model\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "\n",
    "    del checkpoint\n",
    "    if cuda: torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\nevaluating...\\n\")\n",
    "    \n",
    "    B = time.time()\n",
    "    eval_output, iter_eval = pmr.evaluate(model, d_test, device, args)\n",
    "    B = time.time() - B\n",
    "    \n",
    "    print(eval_output.get_AP())\n",
    "    if iter_eval is not None:\n",
    "        print(\"\\nTotal time of this evaluation: {:.1f} s, speed: {:.1f} imgs/s\".format(B, 1 / iter_eval))\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dataset\", default=\"coco\")\n",
    "    parser.add_argument(\"--data-dir\", default=\"/home/moka/Akash/PyTorch-Simple-MaskRCNN/data\")\n",
    "    parser.add_argument(\"--ckpt-path\", default=\"/home/moka/Akash/PyTorch-Simple-MaskRCNN/maskrcnn_coco-3.pth\")\n",
    "    parser.add_argument(\"--iters\", type=int, default=3) # number of iterations, minus means the entire dataset\n",
    "    args = parser.parse_args([]) # [] is needed if you're using Jupyter Notebook.\n",
    "    \n",
    "    args.use_cuda = True\n",
    "    args.results = os.path.join(os.path.dirname(args.ckpt_path), \"maskrcnn_results.pth\")\n",
    "    \n",
    "    main(args)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'training'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m output_onnx_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Convert the model to ONNX format\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mconvert_to_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_onnx_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mconvert_to_onnx\u001b[0;34m(ckpt_path, input_size, output_onnx_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(input_size)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Export the PyTorch model to ONNX format\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_onnx_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Akash/PyTorch-Simple-MaskRCNN/maskrcnn38/lib/python3.8/site-packages/torch/onnx/__init__.py:316\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03mExports a model into ONNX format. If ``model`` is not a\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m:class:`torch.jit.ScriptModule` nor a :class:`torch.jit.ScriptFunction`, this runs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m    model to the file ``f`` even if this is raised.\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m_retain_param_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstrip_doc_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_onnx_checker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_external_data_format\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Akash/PyTorch-Simple-MaskRCNN/maskrcnn38/lib/python3.8/site-packages/torch/onnx/utils.py:107\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_external_data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`use_external_data_format\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and ignored. Will be removed in next \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch release. The code will work as it is False if models are not larger than 2GB, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise set to False because of size limits imposed by Protocol Buffers.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_external_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_external_data_format\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Akash/PyTorch-Simple-MaskRCNN/maskrcnn38/lib/python3.8/site-packages/torch/onnx/utils.py:709\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, use_external_data_format, onnx_shape_inference)\u001b[0m\n\u001b[1;32m    707\u001b[0m _set_opset_version(opset_version)\n\u001b[1;32m    708\u001b[0m _set_operator_export_type(operator_export_type)\n\u001b[0;32m--> 709\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m select_model_mode_for_export(model, training):\n\u001b[1;32m    710\u001b[0m     val_keep_init_as_ip \u001b[38;5;241m=\u001b[39m _decide_keep_init_as_input(keep_initializers_as_inputs,\n\u001b[1;32m    711\u001b[0m                                                      operator_export_type,\n\u001b[1;32m    712\u001b[0m                                                      opset_version)\n\u001b[1;32m    713\u001b[0m     val_add_node_names \u001b[38;5;241m=\u001b[39m _decide_add_node_names(add_node_names, operator_export_type)\n",
      "File \u001b[0;32m/usr/lib/python3.8/contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Akash/PyTorch-Simple-MaskRCNN/maskrcnn38/lib/python3.8/site-packages/torch/onnx/utils.py:42\u001b[0m, in \u001b[0;36mselect_model_mode_for_export\u001b[0;34m(model, mode)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_model_mode_for_export\u001b[39m(model, mode):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction):\n\u001b[0;32m---> 42\u001b[0m         is_originally_training \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m             mode \u001b[38;5;241m=\u001b[39m TrainingMode\u001b[38;5;241m.\u001b[39mEVAL\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'training'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx as onnx\n",
    "import torchvision.models as models\n",
    "\n",
    "def convert_to_onnx(ckpt_path, input_size, output_onnx_path):\n",
    "    # Load the PyTorch model from the .pth file\n",
    "    model = torch.load(ckpt_path)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    # model.eval()\n",
    "\n",
    "    # Create dummy input tensor with the specified size\n",
    "    dummy_input = torch.randn(input_size)\n",
    "\n",
    "    # Export the PyTorch model to ONNX format\n",
    "    onnx.export(model, dummy_input, output_onnx_path, verbose=True, input_names=[\"input\"], output_names=[\"output\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the path to the .pth file\n",
    "    ckpt_path = \"/home/moka/Akash/PyTorch-Simple-MaskRCNN/maskrcnn_coco-3.pth\"\n",
    "\n",
    "    # Specify the input size (replace this with the actual input size of your model)\n",
    "    input_size = (3, 224, 224)\n",
    "\n",
    "    # Specify the output path for the ONNX file\n",
    "    output_onnx_path = \"model.onnx\"\n",
    "\n",
    "    # Convert the model to ONNX format\n",
    "    convert_to_onnx(ckpt_path, input_size, output_onnx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maskrcnn38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
